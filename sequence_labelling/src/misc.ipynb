{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3e408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import training_params\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "324eeea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/train.csv', encoding=\"utf-8\")\n",
    "\n",
    "enc_label = preprocessing.LabelEncoder()\n",
    "df.loc[:, \"label\"] = enc_label.fit_transform(df[\"label\"])\n",
    "\n",
    "sentences = df.groupby(\"sentence\")[\"word\"].apply(list).values\n",
    "labels = df.groupby(\"sentence\")[\"label\"].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "abdbb307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>म्यूरियल</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>अब</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>बीस</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>साल</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>की</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence      word  label\n",
       "0         1  म्यूरियल      2\n",
       "1         1        अब      2\n",
       "2         1       बीस      2\n",
       "3         1       साल      2\n",
       "4         1        की      2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6bd15316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['म्यूरियल', 'अब', 'बीस', 'साल', 'की', 'हो', 'गई', 'है']),\n",
       "       list(['म्यूरियल', 'अब', 'बीस', 'साल', 'की', 'है']),\n",
       "       list(['मैं', 'इस', 'दुनिया', 'में', 'शिक्षा', 'पर', 'बहुत', 'निराश', 'हूँ']),\n",
       "       list(['वैसा', 'नही', 'होगा']),\n",
       "       list(['मुझें', 'तुम्हारी', 'याद', 'आ', 'रही', 'है']),\n",
       "       list(['तुम्हें', 'सोना', 'चाहिए']),\n",
       "       list(['आपको', 'सोना', 'चाहिए']),\n",
       "       list(['मुझे', 'जीव', 'विज्ञान', 'कभी', 'भी', 'पसंद', 'नहीं', 'था']),\n",
       "       list(['मैं', 'नहीं', 'हूँ', 'तुम', 'हो'])], dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "748746ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([2, 2, 2, 2, 2, 2, 2, 4]), list([2, 2, 2, 2, 2, 4]),\n",
       "       list([3, 2, 2, 2, 2, 2, 2, 2, 4]), list([3, 2, 4]),\n",
       "       list([2, 2, 2, 2, 2, 4]), list([3, 2, 4]), list([3, 2, 4]),\n",
       "       list([3, 2, 2, 2, 2, 2, 2, 4]), list([3, 2, 0, 2, 1])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2268f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(list(enc_label.classes_))\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6302acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "   (\n",
    "        train_sentences,\n",
    "        test_sentences,\n",
    "        train_labels,\n",
    "        test_labels,\n",
    "    ) = model_selection.train_test_split(sentences, labels, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "67704948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['म्यूरियल', 'अब', 'बीस', 'साल', 'की', 'है']),\n",
       "       list(['तुम्हें', 'सोना', 'चाहिए']),\n",
       "       list(['म्यूरियल', 'अब', 'बीस', 'साल', 'की', 'हो', 'गई', 'है']),\n",
       "       list(['मैं', 'नहीं', 'हूँ', 'तुम', 'हो']),\n",
       "       list(['मैं', 'इस', 'दुनिया', 'में', 'शिक्षा', 'पर', 'बहुत', 'निराश', 'हूँ']),\n",
       "       list(['मुझें', 'तुम्हारी', 'याद', 'आ', 'रही', 'है']),\n",
       "       list(['वैसा', 'नही', 'होगा']), list(['आपको', 'सोना', 'चाहिए'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67a527fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['मुझे', 'जीव', 'विज्ञान', 'कभी', 'भी', 'पसंद', 'नहीं', 'था'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e4773ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([2, 2, 2, 2, 2, 4]), list([3, 2, 4]),\n",
       "       list([2, 2, 2, 2, 2, 2, 2, 4]), list([3, 2, 0, 2, 1]),\n",
       "       list([3, 2, 2, 2, 2, 2, 2, 2, 4]), list([2, 2, 2, 2, 2, 4]),\n",
       "       list([3, 2, 4]), list([3, 2, 4])], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdf2cdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([3, 2, 2, 2, 2, 2, 2, 4])], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f992e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctuationDataset:\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        labels = self.labels[item]\n",
    "\n",
    "        ids = []\n",
    "        target_labels = []\n",
    "\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = training_params.TOKENIZER.encode(\n",
    "                s,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "            target_labels.extend([labels[i]] * input_len)\n",
    "\n",
    "        ids = ids[:training_params.MAX_LEN - 2]\n",
    "        target_labels = target_labels[:training_params.MAX_LEN - 2]\n",
    "\n",
    "        ids = [101] + ids + [102]\n",
    "        target_labels = [0] + target_labels + [0]\n",
    "\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "\n",
    "        padding_len = training_params.MAX_LEN - len(ids)\n",
    "\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_labels = target_labels + ([0] * padding_len)\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_tag\": torch.tensor(target_labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdd241eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PunctuationDataset(texts=train_sentences, labels=train_labels)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=training_params.TRAIN_BATCH_SIZE,\n",
    "                                                num_workers=4)\n",
    "\n",
    "valid_dataset = PunctuationDataset(texts=test_sentences, labels=test_labels)\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=training_params.VALID_BATCH_SIZE,\n",
    "                                                num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9eadd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(output, target, mask, num_labels):\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1\n",
    "    active_logits = output.view(-1, num_labels)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(cross_entropy_loss.ignore_index).type_as(target)\n",
    "    )\n",
    "    loss = cross_entropy_loss(active_logits, active_labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class PunctuationModel(nn.Module):\n",
    "    def __init__(self, num_tag):\n",
    "        super(PunctuationModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "        self.bert = training_params.MODEL\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out_tag = nn.Linear(768, self.num_tag)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, target_tag):\n",
    "        o1, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        bo_tag = self.bert_drop(o1)\n",
    "\n",
    "        tag = self.out_tag(bo_tag)\n",
    "\n",
    "        loss_tag = loss_function(tag, target_tag, mask, self.num_tag)\n",
    "\n",
    "        loss = loss_tag\n",
    "        return tag, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6a99b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PunctuationModel(\n",
       "  (bert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (bert_drop): Dropout(p=0.3, inplace=False)\n",
       "  (out_tag): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model = PunctuationModel(num_tag=num_labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63d3d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "num_train_steps = int(len(train_sentences) / training_params.TRAIN_BATCH_SIZE * training_params.EPOCHS)\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ec9b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        final_loss += loss.item()\n",
    "    return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9403d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ead51f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('ids', tensor([[  101, 30299,   367, 28689,   311,  1209, 26448,  1883,  4384,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])), ('mask', tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])), ('token_type_ids', tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])), ('target_tag', tensor([[0, 2, 2, 2, 2, 2, 2, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9554e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in eg.items():\n",
    "    eg[k] = v.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    _, loss = model(**eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2241ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = model(**eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f9c59a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 5])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4e6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76208e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('ai4bharat/indic-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7ce639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertForTokenClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']\n",
      "- This IS expected if you are initializing AlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AlbertForTokenClassification.from_pretrained('ai4bharat/indic-bert',\n",
    "                                                   num_labels=5,\n",
    "                                                   output_attentions=False,\n",
    "                                                   output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "245431bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForTokenClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "762700d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"आपका स्वागत हैं\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7112afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deee5f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁आपका', '▁स', 'वागत', '▁हैं']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"आपका स्वागत हैं\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26e6b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5100139a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(1.5914, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.0390,  0.0911, -0.0054, -0.0415,  0.0544],\n",
       "         [ 0.0627, -0.0181, -0.3403,  0.0879,  0.2391],\n",
       "         [-0.0307, -0.0847, -0.0485, -0.0288,  0.0319],\n",
       "         [-0.1493,  0.0654,  0.0814,  0.1902, -0.0588],\n",
       "         [-0.2060, -0.0092, -0.0335,  0.1145, -0.1751],\n",
       "         [ 0.0390,  0.0911, -0.0054, -0.0415,  0.0544]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee13ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b949cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6150b383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0390,  0.0911, -0.0054, -0.0415,  0.0544],\n",
       "         [ 0.0627, -0.0181, -0.3403,  0.0879,  0.2391],\n",
       "         [-0.0307, -0.0847, -0.0485, -0.0288,  0.0319],\n",
       "         [-0.1493,  0.0654,  0.0814,  0.1902, -0.0588],\n",
       "         [-0.2060, -0.0092, -0.0335,  0.1145, -0.1751],\n",
       "         [ 0.0390,  0.0911, -0.0054, -0.0415,  0.0544]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0b002cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 5])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb50bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
