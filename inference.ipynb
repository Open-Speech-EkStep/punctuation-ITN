{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3d3640",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d8b221267e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlbertForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minference_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AlbertForTokenClassification, AlbertTokenizer\n",
    "import numpy as np\n",
    "from utils import process_data\n",
    "import inference_params\n",
    "import json\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7962d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_path = inference_params.LABEL_ENCODER_PATH\n",
    "tag_values = ['none ', 'none', 'exclamation', 'comma', 'viram', 'PAD']\n",
    "with open(label_encoder_path) as label_encoder:\n",
    "    train_encoder = json.load(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7f72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('ai4bharat/indic-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17880f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,   2358,   4962,   5695,   1134,   3132,     23,    219,   1985,\n",
       "            277,   7346,   2326, 180153,    419,      3]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_sentence = \"विकिपीडिया सभी विषयों पर प्रामाणिक और उपयोग परिवर्तन व पुनर्वितरण के लिए स्वतन्त्र ज्ञानकोश बनाने का एक बहुभाषीय प्रकल्प है\"\n",
    "test_sentence = \"अमेरिका समेत अन्य देशों से जो मदद भारत पहुंची उसका क्या हुआ\"\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56043ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertForTokenClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']\n",
      "- This IS expected if you are initializing AlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AlbertForTokenClassification.from_pretrained('ai4bharat/indic-bert',\n",
    "                                                     num_labels=len(train_encoder),\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False)\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    print(\"Using \", torch.cuda.device_count(), \"GPUs\")\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1bfafdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(inference_params.CHECKPOINT_PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0baf85c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): AlbertForTokenClassification(\n",
       "    (albert): AlbertModel(\n",
       "      (embeddings): AlbertEmbeddings(\n",
       "        (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (encoder): AlbertTransformer(\n",
       "        (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "        (albert_layer_groups): ModuleList(\n",
       "          (0): AlbertLayerGroup(\n",
       "            (albert_layers): ModuleList(\n",
       "              (0): AlbertLayer(\n",
       "                (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (attention): AlbertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                  (output_dropout): Dropout(p=0, inplace=False)\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                )\n",
       "                (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "777828f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a88a2bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64faa8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hyp': 0, 'qm': 1, 'comma': 2, 'end': 3, 'blank': 4, 'ex': 5, 'PAD': 6}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed2bbeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁अमेरिका',\n",
       " '▁समेत',\n",
       " '▁अन',\n",
       " 'य',\n",
       " '▁देशों',\n",
       " '▁से',\n",
       " '▁जो',\n",
       " '▁मदद',\n",
       " '▁भारत',\n",
       " '▁पहुंची',\n",
       " '▁उसका',\n",
       " '▁कया',\n",
       " '▁हुआ',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ed289d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(label_indices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d665343",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"▁\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[1:]\n",
    "    else:\n",
    "        new_labels.append(list(train_encoder.keys())[list(train_encoder.values()).index(label_idx)])\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b938c888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank\t[CLS]अमेरिकासमेतअन\n",
      "blank\tयदेशोंसेजोमददभारतपहुंचीउसकाकयाहुआ\n",
      "blank\t[SEP]\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t{}\".format(label, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "848cafd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-38-3b665b79f3ad>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-3b665b79f3ad>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"▁\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[1:]\n",
    "    elif not token.startswith(\"▁\"):\n",
    "        \n",
    "    else:\n",
    "        new_labels.append(list(train_encoder.keys())[list(train_encoder.values()).index(label_idx)])\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b42089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank\t[CLS]अमेरिकासमेतअन\n",
      "blank\tयदेशोंसेजोमददभारतपहुंचीउसकाकयाहुआ\n",
      "blank\t[SEP]\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t{}\".format(label, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21cf4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['अमेरिका', 'समेत', 'अनय', 'देशों', 'से', 'जो', 'मदद', 'भारत', 'पहुंची', 'उसका', 'कया', 'हुआ']\n",
      "['blank', 'blank', 'blank', 'blank', 'blank', 'blank', 'blank', 'blank', 'blank', 'blank', 'blank', 'qm']\n"
     ]
    }
   ],
   "source": [
    "new_tokens = []\n",
    "new_labels = []\n",
    "for i in range(1, len(tokens)-1):\n",
    "    if tokens[i].startswith(\"▁\"):\n",
    "        current_word = tokens[i][1:]\n",
    "        new_labels.append(list(train_encoder.keys())[list(train_encoder.values()).index(label_indices[0][i])])\n",
    "        for j in range(i+1, len(tokens)-1):\n",
    "            if not tokens[j].startswith(\"▁\"):\n",
    "                current_word = current_word + tokens[j]\n",
    "            if tokens[j].startswith(\"▁\"):\n",
    "                break\n",
    "        new_tokens.append(current_word)\n",
    "print(new_tokens)\n",
    "print(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c83b3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = test_sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2a49144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sentence)==len(new_tokens)==len(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40dd9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence))\n",
    "print(len(new_tokens))\n",
    "print(len(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cc8a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_dict = {'hyp': '-', 'qm': '? ', 'comma': ', ', 'end': '। ', 'blank': ' ', 'ex': '! '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c44f1961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'अमेरिका समेत अन्य देशों से जो मदद भारत पहुंची उसका क्या हुआ? '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''\n",
    "for word, punctuation in zip(sentence, new_labels):\n",
    "    s = s + word + punctuation_dict[punctuation]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4271d9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['अमेरिका?', 'समेत ']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''\n",
    "[s+word+punctuation for word, punctuation in zip(sentence, punctuation_dict[punctuation])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435ca92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
